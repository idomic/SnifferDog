{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SHARE Data in the Wide World\n",
    "\n",
    "This notebook will focus on how to export SHARE data into different formats, and how to query SHARE for specific information from your institution, say from a list of names or from a list of emails or ORCIDs that act as reseearcher identifiers.\n",
    "\n",
    "\n",
    "## Exporting a DataFrame to csv and Excel\n",
    "\n",
    "When doing an aggregation on SHARE data, it might be beneficial to export the data to a format that is easier to widely distribute, such as a csv file or and Excel file.\n",
    "\n",
    "First, we'll do a SHARE aggregation query for documents from each source that have a description, turn it into a pandas DataFrame, and export the data into both csv and Excel formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas is a python library that is used for data manipulation and analysis -- good for numbers + time series.\n",
    "    # Pandas gives us some extra data structures (arrays are data structures, for example) which is nice\n",
    "    # We are calling Pandas pd by using the \"as\" -- locally, we know Pandas as pd\n",
    "    # Helpful Links:\n",
    "        # https://en.wikipedia.org/wiki/Pandas_(software)\n",
    "        # http://pandas.pydata.org/ \n",
    "import pandas as pd\n",
    "\n",
    "# Sharepa is a python client for  browsing and analyzing SHARE data specifically using elasticsearch querying.\n",
    "    # We can use this to aggregate, graph, and analyze the data. \n",
    "    # Helpful Links:\n",
    "        # https://github.com/CenterForOpenScience/sharepa\n",
    "        # https://pypi.python.org/pypi/sharepa\n",
    "from sharepa import ShareSearch\n",
    "\n",
    "#When we say from X import Y, we are saying \"of all the things in this python library, import only this\n",
    "from sharepa.helpers import pretty_print\n",
    "\n",
    "description_search = ShareSearch()\n",
    "\n",
    "# exists -- a type of query, will accept a lucene query string\n",
    "    # Lucene supports fielded data. When performing a search you can either specify a field, or use the default field. \n",
    "    # The field names and default field is implementation specific.\n",
    "# field = description -- This lucene query string will find all documents that don't have a description\n",
    "description_search = description_search.query(\n",
    "    'exists', \n",
    "    field='description',\n",
    ")\n",
    "\n",
    "# here we are aggregating all the entries by source\n",
    "description_search.aggs.bucket(\n",
    "    'sources',  # Every aggregation needs a name\n",
    "    'significant_terms',  # There are many kinds of aggregations\n",
    "    field='sources',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    min_doc_count=0,\n",
    "    percentage={}, # Will make the score value the percentage of all results (doc_count/bg_count)\n",
    "    size=0\n",
    ")\n",
    "\n",
    "description_results = description_search.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a dataframe using Pandas (what we call pd) that aggregates the results\n",
    "description_dataframe = pd.DataFrame(description_results.aggregations.sources.to_dict()['buckets'])\n",
    "\n",
    "# We will add our own \"percent\" column to make things clearer\n",
    "description_dataframe['percent'] = (description_dataframe['score'] * 100)\n",
    "\n",
    "# Let's set the source name as the index, and then drop the old column\n",
    "description_dataframe = description_dataframe.set_index(description_dataframe['key'])\n",
    "description_dataframe = description_dataframe.drop('key', 1)\n",
    "\n",
    "# Finally, we'll show the results!\n",
    "description_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's export this pandas dataframe to a csv file, and to an excel file.\n",
    "\n",
    "The next cell will work when running locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Uncomment the following lines if running locally:\n",
    "\n",
    "description_dataframe.to_csv('SHARE_Counts_with_Descriptions.csv')\n",
    "description_dataframe.to_excel('SHARE_Counts_with_Descriptions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Working with outside data\n",
    "\n",
    "Let's say we had a list of names of researchers that were from a particular University. We're interested in seeing if their full names appear in any sources across the SHARE data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this is a simple list\n",
    "names = [\"Susan Jones\", \"Ravi Patel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#this is a \n",
    "name_search = ShareSearch()\n",
    "\n",
    "# We are searching the entire SHARE dataset for each item in the list we called name, i.e. Susan Jones and Ravi Patel\n",
    "for name in names:\n",
    "    name_search = name_search.query(\n",
    "        {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"contributors.full_name\": {\n",
    "                                \"query\": name, \n",
    "                                \"operator\": \"and\",\n",
    "                                \"type\" : \"phrase\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# We are putting all the results into a new list called name_results\n",
    "# name_search is our original list, and .execute() is a built-in function (one that the library provides, and we\n",
    "    # don't have to write) that puts the results of the loop above into a new list\n",
    "name_results = name_search.execute()\n",
    "\n",
    "# Prints out the number of documents that have those \n",
    "print('There are {} documents with contributors who have any of those names.'.format(name_search.count()))\n",
    "\n",
    "# Just visual queues for us to make it more readable\n",
    "print('Here are the first 10:')\n",
    "print('---------')\n",
    "\n",
    "# Loops over the list called \"name_results\" and prints out 10\n",
    "for result in name_results:\n",
    "    print(\n",
    "        '{} -- with contributors {}'.format(\n",
    "            result.title,\n",
    "            [contributor.full_name for contributor in result.contributors]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we were interested to see an analysis of what sources these names came from, we can add an aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name_search.aggs.bucket(\n",
    "    'sources',  # Every aggregation needs a name\n",
    "    'terms',  # There are many kinds of aggregations, terms is a pretty useful one though\n",
    "    field='sources',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    size=0,  # These are just to make sure we get numbers for all the sources, to make it easier to combine graphs\n",
    "    min_doc_count=1\n",
    ")\n",
    "\n",
    "# We are putting all the results into a new list called name_results\n",
    "# name_search is our original list, and .execute() is a built-in function (one that the library provides, and we\n",
    "    # don't have to write) that puts the results of the loop above into a new list\n",
    "name_results = name_search.execute()\n",
    "\n",
    "# We are aggregating these into a DataFrame from Pandas (which we called pd)\n",
    "pd.DataFrame(name_results.aggregations.sources.to_dict()['buckets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}