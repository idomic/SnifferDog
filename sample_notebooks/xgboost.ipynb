{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial how to use [xgboost](https://github.com/dmlc/xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/dataworkshop/xgboost/\n",
    "# pip install xgboost==0.81\n",
    "# !pip install seaborn --upgrade\n",
    "# !pip install scikit-learn\n",
    "# ! pip install pandas==1.3.0 seaborn==0.12.1 matplotlib\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: scipy in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (from xgboost) (1.9.3)\n",
      "Requirement already satisfied: numpy in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "# https://stackoverflow.com/questions/30667525/importerror-no-module-named-sklearn-cross-validation\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idomi/opt/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "#print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8716799674459557\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print( r2_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do the same for classification problem\n",
    "\n",
    "Tips: use **iris** dataset for this and **f1_score** for measure quality and xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "iris_df = sns.load_dataset('iris')\n",
    "dfm = pd.melt(iris_df, id_vars=[\"species\"])\n",
    "sns.stripplot(data=dfm, x=\"species\", y=\"value\", hue=\"variable\", dodge=True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (15, 8)\n",
    "\n",
    "def ground_truth(x):\n",
    "    \"\"\"Ground truth -- function to approximate\"\"\"\n",
    "    return x*np.sin(x) + 2 * np.sin(2 * x) + np.sin(3 * x)\n",
    "\n",
    "def gen_data(n_samples=200):\n",
    "    \"\"\"generate training and testing data\"\"\"\n",
    "    np.random.seed(15)\n",
    "    X = np.random.uniform(0, 10, size=n_samples)[:, np.newaxis]\n",
    "    y = ground_truth(X.ravel()) + np.random.normal(scale=2, size=n_samples)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_data(100)\n",
    "\n",
    "# plot ground truth\n",
    "x_plot = np.linspace(0, 10, 500)\n",
    "\n",
    "def plot_data(alpha=0.4, s=20):\n",
    "    fig = plt.figure(figsize=FIGSIZE)\n",
    "    gt = plt.plot(x_plot, ground_truth(x_plot), alpha=alpha, label='ground truth')\n",
    "\n",
    "    # plot training and testing data\n",
    "    plt.scatter(X_train, y_train, s=s, alpha=alpha)\n",
    "    plt.scatter(X_test, y_test, s=s, alpha=alpha, color='red')\n",
    "    plt.xlim((0, 10))\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('x')\n",
    "    \n",
    "annotation_kw = {'xycoords': 'data', 'textcoords': 'data',\n",
    "                 'arrowprops': {'arrowstyle': '->', 'connectionstyle': 'arc'}}\n",
    "    \n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data()\n",
    "\n",
    "est = DecisionTreeRegressor(max_depth=1).fit(X_train, y_train)\n",
    "x_pred_1 = est.predict(x_plot[:, np.newaxis])\n",
    "plt.plot(x_plot, x_pred_1, label='RT max_depth=1', color='g', alpha=0.9, linewidth=3)\n",
    "\n",
    "est = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
    "         label='RT max_depth=3', color='g', alpha=0.7, linewidth=2)\n",
    "\n",
    "est = DecisionTreeRegressor(max_depth=10).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
    "         label='RT max_depth=10', color='g', alpha=0.5, linewidth=1)\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data()\n",
    "\n",
    "est = RandomForestRegressor(n_estimators=1, max_depth=1).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='RF n_estimators=1, max_depth=1', color='g', alpha=0.9, linewidth=3)\n",
    "\n",
    "est = RandomForestRegressor(n_estimators=1, max_depth=5).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='RF n_estimators=1, max_depth=5', color='g', alpha=0.7, linewidth=2)\n",
    "\n",
    "est = RandomForestRegressor(n_estimators=5, max_depth=5).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='RF n_estimators=5, max_depth=5', color='g', alpha=0.5, linewidth=1)\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data()\n",
    "\n",
    "est = xgb.XGBRegressor(n_estimators=1, max_depth=5).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='XGB n_estimators=1, max_depth=5', color='g', alpha=0.9, linewidth=3)\n",
    "\n",
    "est = xgb.XGBRegressor(n_estimators=10, max_depth=5).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='XGB n_estimators=10, max_depth=5', color='g', alpha=0.7, linewidth=2)\n",
    "\n",
    "est = xgb.XGBRegressor(n_estimators=100, max_depth=5).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='XGB n_estimators=100, max_depth=5', color='g', alpha=0.5, linewidth=1)\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data()\n",
    "\n",
    "params = [ (1, 100)] #, (10, 100), (100, 100) ]\n",
    "for (i, (n_estimators, max_depth)) in enumerate(params):\n",
    "    \n",
    "    est = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth).fit(X_train, y_train)\n",
    "    plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), label='XGB n_estimators={0}, max_depth={1}'.format(n_estimators, max_depth), color='g', alpha=0.9, linewidth=len(params)-i)\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "n_estimators_slider = IntSlider(min=1, max=1000, step=20, value=30)\n",
    "max_depth_slider = IntSlider(min=1, max=15, step=1, value=3)\n",
    "learning_rate_slider = FloatSlider(min=0.01, max=0.3, step=0.01, value=0.1)\n",
    "subsample_slider = FloatSlider(min=0.1, max=1, step=0.1, value=1.0)\n",
    "\n",
    "gamma_slider = FloatSlider(min=0.1, max=1, step=0.1, value=0)\n",
    "reg_alpha_slider = FloatSlider(min=0.1, max=1, step=0.1, value=0)\n",
    "reg_lambda_slider = FloatSlider(min=0.1, max=1, step=0.1, value=1.0)\n",
    "\n",
    "\n",
    "@interact(n_estimators=n_estimators_slider, max_depth=max_depth_slider, learning_rate=learning_rate_slider,\\\n",
    "         subsample=subsample_slider, gamma=gamma_slider, reg_alpha=reg_alpha_slider, reg_lambda=reg_lambda_slider)\n",
    "def plot(n_estimators, max_depth, learning_rate, subsample, gamma, reg_alpha, reg_lambda):\n",
    "    est = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \\\n",
    "                           subsample=subsample, gamma=gamma, reg_alpha=reg_alpha, reg_lambda=reg_lambda).fit(X_train, y_train)\n",
    "\n",
    "    plot_data()\n",
    "    plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), \\\n",
    "             label='XGB n_estimators={0}, max_depth={1}, learning_rate={2}, subsample={3}, gamma={4}, reg_alpha={5}, reg_lambda={6}'.format(n_estimators, max_depth, learning_rate, subsample, gamma, reg_alpha, reg_lambda),\\\n",
    "             color='g', alpha=0.9, linewidth=len(params)-i)\n",
    "    \n",
    "    plt.legend(loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
